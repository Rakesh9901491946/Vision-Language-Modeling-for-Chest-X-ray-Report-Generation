# ğŸ§  Vision-Language Modeling for Chest X-ray Report Generation

This repository contains the codebase, training scripts, and documentation for our research project:  
**"Vision-Language Modeling for Chest X-ray Report Generation: A Cross-Domain Study Using MIMIC-CXR and IU Datasets"**

---

## ğŸ“Œ Project Summary

This project explores an end-to-end vision-language model that generates clinically meaningful **Impression** sections from paired **frontal and lateral chest X-ray images**, optionally using **Findings** as prompts.

The architecture is a **two-view Vision Transformer (ViT)** encoder fused via **average pooling**, followed by a **BioBART encoderâ€“decoder** conditioned on a **learned image prefix token**. The model is trained and evaluated on the **MIMIC-CXR** and **Indiana University (IU X-ray)** datasets.

---

## ğŸ” Key Features

- âœ… **Two-view ViT encoding** for frontal & lateral image inputs.
- ğŸ”— **Average fusion** of [CLS] tokens from both views.
- ğŸ§© **Prefix projection** for image-text coupling into BioBART.
- ğŸ§  **BioBART decoder** generates section-aware clinical summaries.
- ğŸ§ª **Clinical F1** used as the primary metric for evaluation.
- â™»ï¸ Robust training loop with **staged unfreezing**, prompt conditioning, and inference controls.
- ğŸ§° Ready-to-run on **MIMIC-CXR** and **IU datasets**.

---

## ğŸ§± Architecture

![Architecture Diagram](Images/method_schematic.png)

---

## ğŸ“ Repository Structure

